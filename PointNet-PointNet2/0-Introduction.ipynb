{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction to Point Cloud"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Point cloud represents an object or scene in 3D space defined by the coordinates X, Y, Z values for each point. It may contain additional attributes such as color information (RGB), intensity, etc. The data is either generated by depth sensors, or 3D LiDAR sensors. Point Cloud data can be seen as a geometric data defined by a set containing the points."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " In the following, we visualize and explore the most used public Point Cloud Datasets:\n",
    "\n",
    " (1) ModelNet Dataset: The ModelNet40/10 dataset contains objects (in CAD format) from 40 or 10 classes respectively. The dataset also contains the class labels, and is used for classification. In our use case, we use the 40 classes. The data comes with a <modelnet40>.zip file. \n",
    "\n",
    " The ModelNet dataset can be downloaded using the link, https://modelnet.cs.princeton.edu/\n",
    " \n",
    " (2) S3DIS Dataset: The Stanford 3D Indoor Scene Dataset contains 6 large-scale indoor areas divided into 271 rooms. The dataset represents 13 object categories, and provides the point-wise label for semantic segmentation.\n",
    "\n",
    " The dataset can be downloaded using the link, http://buildingparser.stanford.edu/dataset.html"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ModelNet Dataset \n",
    "#### ***How to download the dataset and prepare it accordingly?***\n",
    "\n",
    "To download the dataset and prepare it for the training, follow the given instruction. \n",
    "\n",
    "1. Go the the link to download the data\n",
    "https://modelnet.cs.princeton.edu\n",
    "\n",
    "2. Unzip the data and place the unzipped directory inside data directory.\n",
    "(Note: the 'data' directory is already created for you. You do not need to create it again. You need to place the unzipped folder inside this directory only.)\n",
    "\n",
    "Note: The data directory name should be checked. It is 'modelnet40_normal_resampled' by default. If it is changed, do check the DATA_PATH in the following block."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "### required modules\n",
    "import torch\n",
    "from dataloaders.ModelNetDataLoader import ModelNetDataLoader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "### parameters to be used in Point Cloud Classification with ModelNet in 1-Classification.ipynb\n",
    "class Args:\n",
    "    use_cpu =False\n",
    "    gpu='0'\n",
    "    batch_size = 24\n",
    "    model='pointnet_cls'\n",
    "    num_category = 40\n",
    "    epoch=200\n",
    "    learning_rate=0.001\n",
    "    num_point=1024\n",
    "    optimizer='Adam'\n",
    "    log_dir = 'runs'\n",
    "    decay_rate=1e-4\n",
    "    use_normals=False\n",
    "    process_data=False\n",
    "    use_uniform_sample=False\n",
    "args = Args()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "### ModelNet40 Dataset (classification) load from the disk\n",
    "\n",
    "data_path = '../data/modelnet40_normal_resampled/'\n",
    "\n",
    "train_dataset = ModelNetDataLoader(root=data_path, args=args,  split='train', process_data=args.process_data)\n",
    "trainDataLoader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=10, drop_last=True)\n",
    "\n",
    "test_dataset = ModelNetDataLoader(root=data_path, args=args,  split='test', process_data=args.process_data)\n",
    "trainDataLoader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=True, num_workers=10, drop_last=True)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The size of train data is 9843\n",
      "The size of test data is 2468\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "### Get data from the dataloader for one iteration \n",
    "\n",
    "dataloader_iterator = iter(trainDataLoader)\n",
    "for i in range(1):\n",
    "    try:\n",
    "        data, target = next(dataloader_iterator)\n",
    "    except StopIteration:\n",
    "        dataloader_iterator = iter(trainDataLoader)\n",
    "        data, target = next(dataloader_iterator)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "### Get the data and visualize it\n",
    "import pyvista as pv\n",
    "pv.set_jupyter_backend('ipygany')\n",
    "import numpy as np\n",
    "from utilities.data_description import class_names, class_dist\n",
    "\n",
    "points1 = data\n",
    "points = data.numpy()\n",
    "class_label = target\n",
    "classes = target.numpy()\n",
    "\n",
    "print('batch_size:', args.batch_size) ## check batch size: (default: 24)\n",
    "### Choose i < batch_size \n",
    "i = 12\n",
    "points = points[i]\n",
    "class_label = class_label[i]\n",
    "\n",
    "color = np.ones((points.shape[0], 1))\n",
    "color = color * classes\n",
    "\n",
    "data_plt = pv.PolyData(points)\n",
    "data_plt['color'] = color \n",
    "data_plt.plot()\n",
    "\n",
    "cls_label = class_label.numpy()\n",
    "\n",
    "print('The object belongs to class {} which has {} samples.'.format(class_names[cls_label], str(class_dist[cls_label])))\n",
    "\n",
    "print('Point Cloud Data shape: {}.'.format(points1.shape))\n",
    "print('Label shape: {}.'.format(target.shape))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "batch_size: 24\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bfaaf43afce46bcac183745ecbba7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AppLayout(children=(VBox(children=(HTML(value='<h3>color</h3>'), Dropdown(description='Colormap:', options={'B…"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The object belongs to class table which has 492 samples.\n",
      "Point Cloud Data shape: torch.Size([24, 1024, 3]).\n",
      "Label shape: torch.Size([24]).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. S3DIS Data\n",
    "\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "### import required modules\n",
    "import torch\n",
    "from dataloaders.S3DISDataLoader import S3DISDataset\n",
    "import time\n",
    "import numpy as np\n",
    "import os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "### parameters to be used in Point Cloud Segmentation \n",
    "# with ModelNet in 2-Segmentation.ipynb\n",
    "class Args:\n",
    "    gpu='0'\n",
    "    batch_size = 16\n",
    "    model='pointnet_sem_seg'\n",
    "    epoch=32\n",
    "    learning_rate=0.001\n",
    "    num_point=1024\n",
    "    optimizer='Adam'\n",
    "    log_dir = 'runs'\n",
    "    decay_rate=1e-4\n",
    "    npoint = 4096\n",
    "    step_size =10\n",
    "    lr_decay = 0.7\n",
    "    test_area=5\n",
    "args = Args()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "### We require a GPU to train on this dataset\n",
    "# verify that we have a GPU installed\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "### S3DIS Dataset (segmentation) load from the disk\n",
    "root = '../../../Codespace/data/datasets/S3DIS/scenes/'\n",
    "NUM_CLASSES = 13\n",
    "NUM_POINT = args.npoint\n",
    "BATCH_SIZE = args.batch_size\n",
    "\n",
    "print(\"start loading training data ...\")\n",
    "TRAIN_DATASET = S3DISDataset(split='train', data_root=root, num_point=NUM_POINT, test_area=args.test_area, block_size=1.0, sample_rate=1.0, transform=None)\n",
    "\n",
    "trainDataLoader = torch.utils.data.DataLoader(TRAIN_DATASET, batch_size=BATCH_SIZE, shuffle=True, num_workers=10,\n",
    "                                                pin_memory=True, drop_last=True,\n",
    "                                                worker_init_fn=lambda x: np.random.seed(x + int(time.time())))\n",
    "\n",
    "\n",
    "TEST_DATASET = S3DISDataset(split='test', data_root=root, num_point=NUM_POINT, test_area=args.test_area, block_size=1.0, sample_rate=1.0, transform=None)\n",
    "\n",
    "testDataLoader = torch.utils.data.DataLoader(TEST_DATASET, batch_size=BATCH_SIZE, shuffle=True, num_workers=10,\n",
    "                                                pin_memory=True, drop_last=True,\n",
    "                                                worker_init_fn=lambda x: np.random.seed(x + int(time.time())))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "start loading training data ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 204/204 [00:16<00:00, 12.54it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "47576 samples in train set.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 67/67 [00:07<00:00,  9.09it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "18822 samples in test set.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "### Get data from the dataloader for one iteration \n",
    "\n",
    "dataloader_iterator = iter(trainDataLoader)\n",
    "for i in range(1):\n",
    "    try:\n",
    "        data, target = next(dataloader_iterator)\n",
    "    except StopIteration:\n",
    "        dataloader_iterator = iter(trainDataLoader)\n",
    "        data, target = next(dataloader_iterator)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "### Get the data and visualize it\n",
    "import pyvista as pv\n",
    "pv.set_jupyter_backend('ipygany')\n",
    "import numpy as np\n",
    "from utilities.data_description import classes_s3dis\n",
    "\n",
    "points1 = data\n",
    "points = data.numpy()\n",
    "points = points[:, :, 0:3]\n",
    "class_labels = target\n",
    "classes = class_labels.numpy()\n",
    "\n",
    "print('batch_size:', args.batch_size)\n",
    "### Visualize the data: Choose value of i < batch_size\n",
    "i = 12  # check the batch_size before assinging a value to i (default:16)\n",
    "points = points[i]\n",
    "color = classes[i]\n",
    "data_plt = pv.PolyData(points)\n",
    "data_plt.points *= 10\n",
    "class_color = color.astype(int)\n",
    "plotter = pv.Plotter()\n",
    "plotter.add_mesh(data_plt, scalars=class_color)\n",
    "plotter.show()\n",
    "\n",
    "print('Point Cloud Data shape: {}.'.format(points1.shape))\n",
    "print('Label shape: {}.'.format(target.shape))\n",
    "\n",
    "cls_lbl = classes.flatten()\n",
    "cls_lbl = set(cls_lbl)\n",
    "cls_lbl = list(cls_lbl)\n",
    "cls_lbl = [int(x) for x in cls_lbl]\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "batch_size: 16\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/farai/.anaconda3/envs/cumulusworkshop/lib/python3.8/site-packages/traittypes/traittypes.py:97: UserWarning: Given trait value dtype \"float64\" does not match required type \"float64\". A coerced copy has been created.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d490ef619cd4ea1ae2ca54352024388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "AppLayout(children=(VBox(children=(HTML(value='<h3></h3>'), Dropdown(description='Colormap:', options={'BrBG':…"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Point Cloud Data shape: torch.Size([16, 4096, 9]).\n",
      "Label shape: torch.Size([16, 4096]).\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "### The classes which are present in the file\n",
    "for i in range(len(cls_lbl)):\n",
    "    cls_name = cls_lbl[i]\n",
    "    cls_names = classes_s3dis[cls_name]\n",
    "    print('The classes are {} : {}'.format(cls_name,cls_names))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The classes are 0:Ceiling\n",
      "The classes are 1:Floor\n",
      "The classes are 2:Wall\n",
      "The classes are 3:Beam\n",
      "The classes are 4:Column\n",
      "The classes are 5:Window\n",
      "The classes are 6:Door\n",
      "The classes are 7:Table\n",
      "The classes are 8:Chair\n",
      "The classes are 9:Sofa\n",
      "The classes are 12:Clutter\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Point Cloud and Machine Learning\n",
    "\n",
    "Unlike images or texts, point clouds are unordered. The Convolutional Neural Networks that works on images or texts can not be directly applied to point cloud data. Modern machine learning methods solve this issue by transforming point cloud data to other representations: projections (rendering) for 2D CNNs or voxelization for 3D CNNs. This conversion is time-consuming for the training of a neural network (even in milliseconds). In this conversion process, the point cloud data may lose some of the features (of point cloud). \n",
    "Charles Qi et al. proposed PointNet to solve this issue by considering point clouds as a set of points. This network directly consumes the point clouds and is invariant to the points permutations. This unified architecture is well presented with different vision tasks for point clouds such as classification, segmentation, and part-segmentation. Later, PointNet++ was introduced by the same authors, which is an extension of PointNet. Unlike PointNet, it tries to accumulate the local features (point features) for classification."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following, we will see how the PointNet architecture looks like. We will try to understand the intuition behind the network and what it does. This network can be used for features extraction from raw point cloud data.\n",
    "\n",
    "\n",
    "![image](../.description/pointnet.png)\n",
    "\n",
    "\n",
    "Reference: Qi, C. R., Su, H., Mo, K., & Guibas, L. J. (2017). Pointnet: Deep learning on point sets for 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 652-660)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# import the PointNet semantic segmentation model\n",
    "from models.pointnet_sem_seg import get_model, get_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "NUM_CLASSES = 13\n",
    "\n",
    "classifier = get_model(NUM_CLASSES).cuda()\n",
    "print(classifier)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "get_model(\n",
      "  (feat): PointNetEncoder(\n",
      "    (stn): STN3d(\n",
      "      (conv1): Conv1d(9, 64, kernel_size=(1,), stride=(1,))\n",
      "      (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "      (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
      "      (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (fc3): Linear(in_features=256, out_features=9, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (conv1): Conv1d(9, 64, kernel_size=(1,), stride=(1,))\n",
      "    (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "    (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (fstn): STNkd(\n",
      "      (conv1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "      (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "      (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
      "      (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "      (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "      (fc3): Linear(in_features=256, out_features=4096, bias=True)\n",
      "      (relu): ReLU()\n",
      "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (conv1): Conv1d(1088, 512, kernel_size=(1,), stride=(1,))\n",
      "  (conv2): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
      "  (conv3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
      "  (conv4): Conv1d(128, 13, kernel_size=(1,), stride=(1,))\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parameters in the PointNet Network\n",
    "In the following, we will see the layers, output shape, and the number of parameters per layer. We will also see the total number of parameters contained in the network."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "### import the required modules\n",
    "from torchsummary import summary\n",
    "classifier = classifier.cuda()\n",
    "summary(classifier, (9, 1024))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1             [-1, 64, 1024]             640\n",
      "       BatchNorm1d-2             [-1, 64, 1024]             128\n",
      "            Conv1d-3            [-1, 128, 1024]           8,320\n",
      "       BatchNorm1d-4            [-1, 128, 1024]             256\n",
      "            Conv1d-5           [-1, 1024, 1024]         132,096\n",
      "       BatchNorm1d-6           [-1, 1024, 1024]           2,048\n",
      "            Linear-7                  [-1, 512]         524,800\n",
      "       BatchNorm1d-8                  [-1, 512]           1,024\n",
      "            Linear-9                  [-1, 256]         131,328\n",
      "      BatchNorm1d-10                  [-1, 256]             512\n",
      "           Linear-11                    [-1, 9]           2,313\n",
      "            STN3d-12                 [-1, 3, 3]               0\n",
      "           Conv1d-13             [-1, 64, 1024]             640\n",
      "      BatchNorm1d-14             [-1, 64, 1024]             128\n",
      "           Conv1d-15             [-1, 64, 1024]           4,160\n",
      "      BatchNorm1d-16             [-1, 64, 1024]             128\n",
      "           Conv1d-17            [-1, 128, 1024]           8,320\n",
      "      BatchNorm1d-18            [-1, 128, 1024]             256\n",
      "           Conv1d-19           [-1, 1024, 1024]         132,096\n",
      "      BatchNorm1d-20           [-1, 1024, 1024]           2,048\n",
      "           Linear-21                  [-1, 512]         524,800\n",
      "      BatchNorm1d-22                  [-1, 512]           1,024\n",
      "           Linear-23                  [-1, 256]         131,328\n",
      "      BatchNorm1d-24                  [-1, 256]             512\n",
      "           Linear-25                 [-1, 4096]       1,052,672\n",
      "            STNkd-26               [-1, 64, 64]               0\n",
      "           Conv1d-27            [-1, 128, 1024]           8,320\n",
      "      BatchNorm1d-28            [-1, 128, 1024]             256\n",
      "           Conv1d-29           [-1, 1024, 1024]         132,096\n",
      "      BatchNorm1d-30           [-1, 1024, 1024]           2,048\n",
      "  PointNetEncoder-31  [[-1, 1088, 1024], [-1, 3, 3], [-1, 64, 64]]               0\n",
      "           Conv1d-32            [-1, 512, 1024]         557,568\n",
      "      BatchNorm1d-33            [-1, 512, 1024]           1,024\n",
      "           Conv1d-34            [-1, 256, 1024]         131,328\n",
      "      BatchNorm1d-35            [-1, 256, 1024]             512\n",
      "           Conv1d-36            [-1, 128, 1024]          32,896\n",
      "      BatchNorm1d-37            [-1, 128, 1024]             256\n",
      "           Conv1d-38             [-1, 13, 1024]           1,677\n",
      "================================================================\n",
      "Total params: 3,529,558\n",
      "Trainable params: 3,529,558\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.04\n",
      "Forward/backward pass size (MB): 313415.19\n",
      "Params size (MB): 13.46\n",
      "Estimated Total Size (MB): 313428.69\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The transform networks\n",
    "PointNet utilizes two T-Nets (Transform Networks). The first T-Net is applied to the input data (raw point cloud), while the second T-Net is applied to the features. Thus, the first T-Net is the input Transform Network, and the second T-Net is the feature transform network. These networks output an unconstrained affine transformation.\n",
    "\n",
    "In the following, we will see the network architecture for T-Nets in PointNet. We will also see the number of layers in both the T-Nets with their respective parameters and output shapes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ***T-Net on Inputs***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# import the T-Net input\n",
    "from models.pointnet_utils import STN3d\n",
    "inp_trans = STN3d(channel=3)\n",
    "print(inp_trans)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "STN3d(\n",
      "  (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
      "  (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "  (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
      "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=9, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "from torchsummary import summary\n",
    "inp_trans_sum = inp_trans.cuda()\n",
    "summary(inp_trans_sum, (3,1024))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1             [-1, 64, 1024]             256\n",
      "       BatchNorm1d-2             [-1, 64, 1024]             128\n",
      "            Conv1d-3            [-1, 128, 1024]           8,320\n",
      "       BatchNorm1d-4            [-1, 128, 1024]             256\n",
      "            Conv1d-5           [-1, 1024, 1024]         132,096\n",
      "       BatchNorm1d-6           [-1, 1024, 1024]           2,048\n",
      "            Linear-7                  [-1, 512]         524,800\n",
      "       BatchNorm1d-8                  [-1, 512]           1,024\n",
      "            Linear-9                  [-1, 256]         131,328\n",
      "      BatchNorm1d-10                  [-1, 256]             512\n",
      "           Linear-11                    [-1, 9]           2,313\n",
      "================================================================\n",
      "Total params: 803,081\n",
      "Trainable params: 803,081\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 19.01\n",
      "Params size (MB): 3.06\n",
      "Estimated Total Size (MB): 22.09\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### ***T-Net on Features***"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "from models.pointnet_utils import STNkd\n",
    "feat_trans = STNkd(k=64)\n",
    "print(feat_trans)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "STNkd(\n",
      "  (conv1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "  (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "  (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
      "  (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=4096, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "from torchsummary import summary\n",
    "feat_trans_sum = feat_trans.cuda()\n",
    "summary(feat_trans_sum, (64,1024))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1             [-1, 64, 1024]           4,160\n",
      "       BatchNorm1d-2             [-1, 64, 1024]             128\n",
      "            Conv1d-3            [-1, 128, 1024]           8,320\n",
      "       BatchNorm1d-4            [-1, 128, 1024]             256\n",
      "            Conv1d-5           [-1, 1024, 1024]         132,096\n",
      "       BatchNorm1d-6           [-1, 1024, 1024]           2,048\n",
      "            Linear-7                  [-1, 512]         524,800\n",
      "       BatchNorm1d-8                  [-1, 512]           1,024\n",
      "            Linear-9                  [-1, 256]         131,328\n",
      "      BatchNorm1d-10                  [-1, 256]             512\n",
      "           Linear-11                 [-1, 4096]       1,052,672\n",
      "================================================================\n",
      "Total params: 1,857,344\n",
      "Trainable params: 1,857,344\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.25\n",
      "Forward/backward pass size (MB): 19.04\n",
      "Params size (MB): 7.09\n",
      "Estimated Total Size (MB): 26.38\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The PointNet Encoder (provides the Global Features)\n",
    "\n",
    "The PointNet encoder extracts the global features from raw point cloud data. However, it fails to consider the local features. In the classification network, the global features are extracted using the encoder. It does not consider the local features. On the other hand, in the semantic segmentation task, the global features from the encoder are concatenated with the point-wise features from the features transform network."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "### import the required PointNet encoder module\n",
    "from models.pointnet_utils import PointNetEncoder\n",
    "encoder = PointNetEncoder(global_feat=True, feature_transform=False, channel=3)\n",
    "print(encoder)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "PointNetEncoder(\n",
      "  (stn): STN3d(\n",
      "    (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
      "    (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "    (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
      "    (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (fc3): Linear(in_features=256, out_features=9, bias=True)\n",
      "    (relu): ReLU()\n",
      "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv1): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n",
      "  (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
      "  (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
      "  (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "from torchsummary import summary\n",
    "encoder_sum = encoder.cuda()\n",
    "summary(encoder_sum, (3,1024))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1             [-1, 64, 1024]             256\n",
      "       BatchNorm1d-2             [-1, 64, 1024]             128\n",
      "            Conv1d-3            [-1, 128, 1024]           8,320\n",
      "       BatchNorm1d-4            [-1, 128, 1024]             256\n",
      "            Conv1d-5           [-1, 1024, 1024]         132,096\n",
      "       BatchNorm1d-6           [-1, 1024, 1024]           2,048\n",
      "            Linear-7                  [-1, 512]         524,800\n",
      "       BatchNorm1d-8                  [-1, 512]           1,024\n",
      "            Linear-9                  [-1, 256]         131,328\n",
      "      BatchNorm1d-10                  [-1, 256]             512\n",
      "           Linear-11                    [-1, 9]           2,313\n",
      "            STN3d-12                 [-1, 3, 3]               0\n",
      "           Conv1d-13             [-1, 64, 1024]             256\n",
      "      BatchNorm1d-14             [-1, 64, 1024]             128\n",
      "           Conv1d-15            [-1, 128, 1024]           8,320\n",
      "      BatchNorm1d-16            [-1, 128, 1024]             256\n",
      "           Conv1d-17           [-1, 1024, 1024]         132,096\n",
      "      BatchNorm1d-18           [-1, 1024, 1024]           2,048\n",
      "================================================================\n",
      "Total params: 946,185\n",
      "Trainable params: 946,185\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.01\n",
      "Forward/backward pass size (MB): 38.01\n",
      "Params size (MB): 3.61\n",
      "Estimated Total Size (MB): 41.63\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f50e1d202e66ca9d1bb86231cb6414771e783b64b793893d086f00fc72b4d7af"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('cumulusworkshop': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}