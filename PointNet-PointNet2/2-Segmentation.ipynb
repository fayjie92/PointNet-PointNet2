{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Scene Segmentation from 3D-Point Clouds Data\n",
    "In this tutorial, we are going to explore semantic segmentation for indoor scenes represented in S3DIS dataset using PointNet.\n",
    "\n",
    "All the theoretical intuitions for the PointNet and the S3DIS dataset is available in [Introduction Notebook](0-Introduction.ipynb)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "### Import the required modules.\n",
    "import os\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from dataloaders.S3DISDataLoader import S3DISDataset\n",
    "from utilities.activation import inplace_relu    ### Saves memory\n",
    "from utilities.data_description import seg_label_to_cat  \n",
    "from utilities.data_manipulation import rotate_point_cloud_z\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following, we define the parameters for our model to train."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class Args:\n",
    "    gpu='0'\n",
    "    batch_size = 16\n",
    "    model='pointnet_sem_seg'\n",
    "    epoch=32\n",
    "    learning_rate=0.001\n",
    "    num_point=1024\n",
    "    optimizer='Adam'\n",
    "    log_dir = 'runs'\n",
    "    decay_rate=1e-4\n",
    "    npoint = 4096\n",
    "    step_size =10\n",
    "    lr_decay = 0.7\n",
    "    test_area=5\n",
    "args = Args()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following block, we define the test function. We will use this function inside our training loop to validate and see the performance of our model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def test(model, loader):    \n",
    "    num_batches = len(loader)\n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    labelweights = np.zeros(NUM_CLASSES)\n",
    "    total_seen_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    total_correct_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    total_iou_deno_class = [0 for _ in range(NUM_CLASSES)]\n",
    "    classifier = model.eval()\n",
    "\n",
    "    print('---- EPOCH %03d EVALUATION ----' % (global_epoch + 1))\n",
    "    for i, (points, target) in tqdm(enumerate(loader), total=len(loader), smoothing=0.9):\n",
    "        points = points.data.numpy()\n",
    "        points = torch.Tensor(points)\n",
    "        points, target = points.float().cuda(), target.long().cuda()\n",
    "        points = points.transpose(2, 1)\n",
    "\n",
    "        seg_pred, trans_feat = classifier(points)\n",
    "        pred_val = seg_pred.contiguous().cpu().data.numpy()\n",
    "        seg_pred = seg_pred.contiguous().view(-1, NUM_CLASSES)\n",
    "\n",
    "        batch_label = target.cpu().data.numpy()\n",
    "        target = target.view(-1, 1)[:, 0]\n",
    "        loss = criterion(seg_pred, target, trans_feat, weights)\n",
    "        loss_sum += loss\n",
    "        pred_val = np.argmax(pred_val, 2)\n",
    "        correct = np.sum((pred_val == batch_label))\n",
    "        total_correct += correct\n",
    "        total_seen += (BATCH_SIZE * NUM_POINT)\n",
    "        tmp, _ = np.histogram(batch_label, range(NUM_CLASSES + 1))\n",
    "        labelweights += tmp\n",
    "\n",
    "        for l in range(NUM_CLASSES):\n",
    "            total_seen_class[l] += np.sum((batch_label == l))\n",
    "            total_correct_class[l] += np.sum((pred_val == l) & (batch_label == l))\n",
    "            total_iou_deno_class[l] += np.sum(((pred_val == l) | (batch_label == l)))\n",
    "\n",
    "    labelweights = labelweights.astype(np.float32) / np.sum(labelweights.astype(np.float32))\n",
    "    mIoU = np.mean(np.array(total_correct_class) / (np.array(total_iou_deno_class, dtype=np.float) + 1e-6))\n",
    "    print('eval mean loss: %f' % (loss_sum / float(num_batches)))\n",
    "    print('eval point avg class IoU: %f' % (mIoU))\n",
    "    print('eval point accuracy: %f' % (total_correct / float(total_seen)))\n",
    "    print('eval point avg class acc: %f' % (\n",
    "        np.mean(np.array(total_correct_class) / (np.array(total_seen_class, dtype=np.float) + 1e-6))))\n",
    "\n",
    "    iou_per_class_str = '------- IoU --------\\n'\n",
    "    for l in range(NUM_CLASSES):\n",
    "        iou_per_class_str += 'class %s weight: %.3f, IoU: %.3f \\n' % (\n",
    "            seg_label_to_cat[l] + ' ' * (14 - len(seg_label_to_cat[l])), labelweights[l - 1],\n",
    "            total_correct_class[l] / float(total_iou_deno_class[l]))\n",
    "\n",
    "    print(iou_per_class_str)\n",
    "    print('Eval mean loss: %f' % (loss_sum / num_batches))\n",
    "    print('Eval accuracy: %f' % (total_correct / float(total_seen)))\n",
    "\n",
    "    return mIoU\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Verify if a Nvidia GPU is available for training the network. Check args.gpu value to define the available GPU in a cluster of GPUs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following, we will load the S3DIS Dataset from the disk. To download the dataset, follow the download and preparation instructions given in [Introduction Notebook](0-Introduction.ipynb). The same notebook also explains the dataset, its classes, and helps to visualizes the scenes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "root = '../data/stanford_indoor3d/'\n",
    "NUM_CLASSES = 13\n",
    "NUM_POINT = args.npoint\n",
    "BATCH_SIZE = args.batch_size\n",
    "\n",
    "print(\"start loading training data ...\")\n",
    "TRAIN_DATASET = S3DISDataset(split='train', data_root=root, num_point=NUM_POINT, test_area=args.test_area, block_size=1.0, sample_rate=1.0, transform=None)\n",
    "print(\"start loading test data ...\")\n",
    "TEST_DATASET = S3DISDataset(split='test', data_root=root, num_point=NUM_POINT, test_area=args.test_area, block_size=1.0, sample_rate=1.0, transform=None)\n",
    "\n",
    "trainDataLoader = torch.utils.data.DataLoader(TRAIN_DATASET, batch_size=BATCH_SIZE, shuffle=True, num_workers=10,\n",
    "                                                pin_memory=True, drop_last=True,\n",
    "                                                worker_init_fn=lambda x: np.random.seed(x + int(time.time())))\n",
    "testDataLoader = torch.utils.data.DataLoader(TEST_DATASET, batch_size=BATCH_SIZE, shuffle=False, num_workers=10,\n",
    "                                                pin_memory=True, drop_last=True)\n",
    "weights = torch.Tensor(TRAIN_DATASET.labelweights).cuda()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "start loading training data ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 204/204 [00:15<00:00, 13.31it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "47576 samples in train set.\n",
      "start loading test data ...\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 67/67 [00:06<00:00, 10.63it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "18822 samples in test set.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following we define the PointNet segmentation model. The insights of PointNet and their different parts are discussed in [Introduction Notebook](0-Introduction.ipynb)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "### load pointnet segmentation model\n",
    "from models.pointnet_sem_seg import get_model, get_loss\n",
    "\n",
    "classifier = get_model(NUM_CLASSES).cuda()\n",
    "criterion = get_loss().cuda()\n",
    "classifier.apply(inplace_relu)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "get_model(\n",
       "  (feat): PointNetEncoder(\n",
       "    (stn): STN3d(\n",
       "      (conv1): Conv1d(9, 64, kernel_size=(1,), stride=(1,))\n",
       "      (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "      (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
       "      (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (fc3): Linear(in_features=256, out_features=9, bias=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv1): Conv1d(9, 64, kernel_size=(1,), stride=(1,))\n",
       "    (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "    (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
       "    (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (fstn): STNkd(\n",
       "      (conv1): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
       "      (conv2): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n",
       "      (conv3): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n",
       "      (fc1): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
       "      (fc3): Linear(in_features=256, out_features=4096, bias=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (bn1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn5): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (conv1): Conv1d(1088, 512, kernel_size=(1,), stride=(1,))\n",
       "  (conv2): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n",
       "  (conv3): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n",
       "  (conv4): Conv1d(128, 13, kernel_size=(1,), stride=(1,))\n",
       "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following, we initialize the optimizer for our model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "if args.optimizer == 'Adam':\n",
    "        optimizer = torch.optim.Adam(\n",
    "            classifier.parameters(),\n",
    "            lr=args.learning_rate,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-08,\n",
    "            weight_decay=args.decay_rate\n",
    "        )\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(classifier.parameters(), \n",
    "    lr=args.learning_rate, momentum=0.9)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following, we define the momentum. Momentum is used along with batch normalization. Momentum helps in reducing the noise in Gradient update term. Batch normalization reduces the coupling of prior layers parameters to the later stage parameters. So, it helps in stablizing the inputs feed to a layer. Both momentum and batch normalization help in faster convergence of the network."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def bn_momentum_adjust(m, momentum):\n",
    "    if isinstance(m, torch.nn.BatchNorm2d) or isinstance(m, torch.nn.BatchNorm1d):\n",
    "        m.momentum = momentum"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following, we will check if there exists a pre-trained weight. If it exists, then we load the weights and resume the training process. If not, then we start the training from the beginning. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "try:\n",
    "    checkpoint = torch.load('save_weights/best_model_segmentation.pth')\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print('Use pretrain model')\n",
    "except:\n",
    "    print('No existing model, starting training from scratch...')\n",
    "    start_epoch = 0"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No existing model, starting training from scratch...\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the following, we initialize the training parameters to measure the performance of the model. Following the literature, we calculate the mIoU (mean intersection over union) to evaluate the model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "## training parameters.\n",
    "\n",
    "LEARNING_RATE_CLIP = 1e-5\n",
    "MOMENTUM_ORIGINAL = 0.1\n",
    "MOMENTUM_DECCAY = 0.5\n",
    "MOMENTUM_DECCAY_STEP = args.step_size\n",
    "\n",
    "global_epoch = 0\n",
    "best_iou = 0"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "for epoch in range(start_epoch, args.epoch):\n",
    "    '''Train on chopped scenes'''\n",
    "    print('**** Epoch %d (%d/%s) ****' % (global_epoch + 1, epoch + 1, args.epoch))\n",
    "    lr = max(args.learning_rate * (args.lr_decay ** (epoch // args.step_size)), LEARNING_RATE_CLIP)\n",
    "    print('Learning rate:%f' % lr)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    momentum = MOMENTUM_ORIGINAL * (MOMENTUM_DECCAY ** (epoch // MOMENTUM_DECCAY_STEP))\n",
    "    if momentum < 0.01:\n",
    "        momentum = 0.01\n",
    "    print('BN momentum updated to: %f' % momentum)\n",
    "    classifier = classifier.apply(lambda x: bn_momentum_adjust(x, momentum))\n",
    "    num_batches = len(trainDataLoader)\n",
    "    total_correct = 0\n",
    "    total_seen = 0\n",
    "    loss_sum = 0\n",
    "    classifier = classifier.train()\n",
    "\n",
    "    for i, (points, target) in tqdm(enumerate(trainDataLoader), total=len(trainDataLoader), smoothing=0.9):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        points = points.data.numpy()\n",
    "        points[:, :, :3] = rotate_point_cloud_z(points[:, :, :3])\n",
    "        points = torch.Tensor(points)\n",
    "        points, target = points.float().cuda(), target.long().cuda()\n",
    "        points = points.transpose(2, 1)\n",
    "\n",
    "        seg_pred, trans_feat = classifier(points)\n",
    "        seg_pred = seg_pred.contiguous().view(-1, NUM_CLASSES)\n",
    "\n",
    "        batch_label = target.view(-1, 1)[:, 0].cpu().data.numpy()\n",
    "        target = target.view(-1, 1)[:, 0]\n",
    "        loss = criterion(seg_pred, target, trans_feat, weights)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred_choice = seg_pred.cpu().data.max(1)[1].numpy()\n",
    "        correct = np.sum(pred_choice == batch_label)\n",
    "        total_correct += correct\n",
    "        total_seen += (BATCH_SIZE * NUM_POINT)\n",
    "        loss_sum += loss\n",
    "    print('Training mean loss: %f' % (loss_sum / num_batches))\n",
    "    print('Training accuracy: %f' % (total_correct / float(total_seen)))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        mIoU = test(classifier.eval(), testDataLoader)\n",
    "\n",
    "        if mIoU >= best_iou:\n",
    "            best_iou = mIoU\n",
    "            print('Save model...')\n",
    "            savepath = 'weights' + '/best_model_segmentation.pth'\n",
    "            print('Saving at %s' % savepath)\n",
    "            state = {\n",
    "                'epoch': epoch,\n",
    "                'class_avg_iou': mIoU,\n",
    "                'model_state_dict': classifier.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }\n",
    "            torch.save(state, savepath)\n",
    "            print('Saving model....')\n",
    "        print('Best mIoU: %f' % best_iou)\n",
    "    global_epoch += 1\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "**** Epoch 1 (1/32) ****\n",
      "Learning rate:0.001000\n",
      "BN momentum updated to: 0.100000\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 2973/2973 [12:03<00:00,  4.11it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training mean loss: 1.112368\n",
      "Training accuracy: 0.674392\n",
      "---- EPOCH 001 EVALUATION ----\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      " 43%|████▎     | 501/1176 [01:24<01:03, 10.67it/s]"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f50e1d202e66ca9d1bb86231cb6414771e783b64b793893d086f00fc72b4d7af"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('cumulusworkshop': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}