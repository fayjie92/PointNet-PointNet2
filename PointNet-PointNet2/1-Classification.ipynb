{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'> 1. PointNet Object Classification using Deep Learning  </font> \n",
    "- Introduction \n",
    "- Network Architecture\n",
    "- Hands-On Experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Introduction\n",
    "Point Cloud is an important geometrical datatype that is canonical (depth, lidar) but irregular. Due to its irregularity, most research works focus on transforming the point cloud data to regular 3D voxel grids (3D-CNN) or collections of images (2D-CNN). However, such transformation leads to various issues, and it becomes voluminous. The point cloud transformation may lead to losing the basic structure (or features) of point cloud data. </font> \n",
    "\n",
    "<font color = 'black'> To end this, PointNet was proposed in 2017 that focuses on learning a model on raw point cloud data. The network is the first one in this area, and basic but it is robust to perturbation and corruption. It is efficient and effective in many point cloud tasks such as object classification, part segmentation and semantic segmentation. \n",
    "\n",
    "In the following, we will see object classification with this method using the ModelNet40 dataset.  It is possible to extend this method it to any custom dataset.\n",
    "\n",
    "For further details, the article is available at: https://arxiv.org/abs/1612.00593 \n",
    "\n",
    "Reference: Qi, Charles R., et al. \"Pointnet: Deep learning on point sets for 3d classification and segmentation.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017. </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Network Architecture: PointNet\n",
    "<img src=\"../../description/pointnet.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Hands-On-Experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from dataloaders.ModelNetDataLoader import ModelNetDataLoader\n",
    "from utilities.data_provide import random_point_dropout, random_scale_point_cloud, shift_point_cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = 'red'> Parameters </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    '''PARAMETERS'''\n",
    "    use_cpu =False\n",
    "    gpu='0'\n",
    "    batch_size = 24\n",
    "    model='pointnet_cls'\n",
    "    num_category = 40\n",
    "    epoch=200\n",
    "    learning_rate=0.001\n",
    "    num_point=1024\n",
    "    optimizer='Adam'\n",
    "    log_dir = 'runs'\n",
    "    decay_rate=1e-4\n",
    "    use_normals=False\n",
    "    process_data=False\n",
    "    use_uniform_sample=False\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Function for the test dataloader. Used inside the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(model, loader, num_class=40):\n",
    "    mean_correct = []\n",
    "    class_acc = np.zeros((num_class, 3))\n",
    "    classifier = model.eval()\n",
    "\n",
    "    for j, (points, target) in tqdm(enumerate(loader), total=len(loader)):\n",
    "\n",
    "        if not args.use_cpu:\n",
    "            points, target = points.cuda(), target.cuda()\n",
    "\n",
    "        points = points.transpose(2, 1)\n",
    "        pred, _ = classifier(points)\n",
    "        pred_choice = pred.data.max(1)[1]\n",
    "\n",
    "        for cat in np.unique(target.cpu()):\n",
    "            classacc = pred_choice[target == cat].eq(target[target == cat].long().data).cpu().sum()\n",
    "            class_acc[cat, 0] += classacc.item() / float(points[target == cat].size()[0])\n",
    "            class_acc[cat, 1] += 1\n",
    "\n",
    "        correct = pred_choice.eq(target.long().data).cpu().sum()\n",
    "        mean_correct.append(correct.item() / float(points.size()[0]))\n",
    "\n",
    "    class_acc[:, 2] = class_acc[:, 0] / class_acc[:, 1]\n",
    "    class_acc = np.mean(class_acc[:, 2])\n",
    "    instance_acc = np.mean(mean_correct)\n",
    "\n",
    "    return instance_acc, class_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inplace Relu saves memory!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inplace_relu(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('ReLU') != -1:\n",
    "        m.inplace=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do require a GPU. Define the GPU value in a cluster of GPUs. By default it is 0 (a single GPU environment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataloader Part: \n",
    "We choose ModelNet_40 dataset for our implementation. It is a classification dataset that consists of 40 classes for household objects. The training and testing split is defined inside the modelnet40_normal_resampled directory. The data represents CAD models which are cleaned inhouse.\n",
    "\n",
    "For our implementation, we are using the model resampled version (Aligned).\n",
    "\n",
    "The aligned dataset is provided by N. Sedaghat, M. Zolfaghari, E. Amiri and T. Brox, the authors of Orientation-boosted Voxel Nets for 3D Object Recognition [8].\n",
    "\n",
    "The CAD models are in Object File Format (OFF). Matlab functions are provide with the dataset to read and visualize OFF files in Princeton Vision Toolkit (PVT).\n",
    "\n",
    "The dataset and the related information is available at: https://modelnet.cs.princeton.edu\n",
    "\n",
    "You can also use ModelNet10 dataset. This dataset consists of 10 classes instead of 40 classes.\n",
    "\n",
    "\n",
    "#### <font color = 'red'> How to download the dataset and prepare it accordingly? </font>\n",
    "\n",
    "To download the dataset and prepare it for the training, follow the given instruction. \n",
    "\n",
    "1. Go the the link to download the data\n",
    "https://modelnet.cs.princeton.edu\n",
    "\n",
    "2. Unzip the data and place the unzipped directory inside data directory.\n",
    "(Note: the 'data' directory is already created for you. You do not need to create it again. You need to place the unzipped folder inside this directory only.)\n",
    "\n",
    "Note: The data directory name should be checked. It is 'modelnet40_normal_resampled' by default. If it is changed, do check the DATA_PATH in the following block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../../data/modelnet40_normal_resampled/'\n",
    "\n",
    "train_dataset = ModelNetDataLoader(root=data_path, args=args,  split='train', process_data=args.process_data)\n",
    "test_dataset = ModelNetDataLoader(root=data_path, args=args, split='test', process_data=args.process_data)\n",
    "trainDataLoader = torch.utils.data.DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=10, drop_last=True)\n",
    "testDataLoader = torch.utils.data.DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Part: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''MODEL LOADING'''\n",
    "num_class = args.num_category\n",
    "from models.pointnet_cls import get_model, get_loss\n",
    "classifier = get_model(num_class, normal_channel=args.use_normals)\n",
    "criterion = get_loss()\n",
    "classifier.apply(inplace_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight save directory and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not args.use_cpu:\n",
    "    classifier = classifier.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "\n",
    "try:\n",
    "    checkpoint = torch.load('weights/best_model_classification.pth')\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    classifier.load_state_dict(checkpoint['model_state_dict'])\n",
    "    print('Use pretrain model')\n",
    "except:\n",
    "    print('No existing model, starting training from scratch...')\n",
    "    start_epoch = 0\n",
    "\n",
    "if args.optimizer == 'Adam':\n",
    "    optimizer = torch.optim.Adam(\n",
    "        classifier.parameters(),\n",
    "        lr=args.learning_rate,\n",
    "        betas=(0.9, 0.999),\n",
    "        eps=1e-08,\n",
    "        weight_decay=args.decay_rate\n",
    "    )\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(classifier.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schedular and Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.7)\n",
    "global_epoch = 0\n",
    "global_step = 0\n",
    "best_instance_acc = 0.0\n",
    "best_class_acc = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start training...')\n",
    "for epoch in range(start_epoch, args.epoch):\n",
    "    print('Epoch %d (%d/%s):' % (global_epoch + 1, epoch + 1, args.epoch))\n",
    "    mean_correct = []\n",
    "    classifier = classifier.train()\n",
    "\n",
    "    scheduler.step()\n",
    "    for batch_id, (points, target) in tqdm(enumerate(trainDataLoader, 0), total=len(trainDataLoader), smoothing=0.9):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        points = points.data.numpy()\n",
    "        points = random_point_dropout(points)\n",
    "        points[:, :, 0:3] = random_scale_point_cloud(points[:, :, 0:3])\n",
    "        points[:, :, 0:3] = shift_point_cloud(points[:, :, 0:3])\n",
    "        points = torch.Tensor(points)\n",
    "        points = points.transpose(2, 1)\n",
    "\n",
    "        if not args.use_cpu:\n",
    "            points, target = points.cuda(), target.cuda()\n",
    "\n",
    "        pred, trans_feat = classifier(points)\n",
    "        loss = criterion(pred, target.long(), trans_feat)\n",
    "        pred_choice = pred.data.max(1)[1]\n",
    "\n",
    "        correct = pred_choice.eq(target.long().data).cpu().sum()\n",
    "        mean_correct.append(correct.item() / float(points.size()[0]))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        global_step += 1\n",
    "\n",
    "    train_instance_acc = np.mean(mean_correct)\n",
    "    print('Train Instance Accuracy: %f' % train_instance_acc)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        instance_acc, class_acc = test(classifier.eval(), testDataLoader, num_class=num_class)\n",
    "\n",
    "        if (instance_acc >= best_instance_acc):\n",
    "            best_instance_acc = instance_acc\n",
    "            best_epoch = epoch + 1\n",
    "\n",
    "        if (class_acc >= best_class_acc):\n",
    "            best_class_acc = class_acc\n",
    "        print('Test Instance Accuracy: %f, Class Accuracy: %f' % (instance_acc, class_acc))\n",
    "        print('Best Instance Accuracy: %f, Class Accuracy: %f' % (best_instance_acc, best_class_acc))\n",
    "\n",
    "        if (instance_acc >= best_instance_acc):\n",
    "            print('Save model...')\n",
    "            savepath = 'weights/' + 'best_model_classification.pth'\n",
    "            print('Saving at %s' % savepath)\n",
    "            state = {\n",
    "                'epoch': best_epoch,\n",
    "                'instance_acc': instance_acc,\n",
    "                'class_acc': class_acc,\n",
    "                'model_state_dict': classifier.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }\n",
    "            torch.save(state, savepath)\n",
    "        global_epoch += 1\n",
    "\n",
    "print('End of training...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f50e1d202e66ca9d1bb86231cb6414771e783b64b793893d086f00fc72b4d7af"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
